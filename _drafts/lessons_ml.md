## autencoder compression

Idea of going from verbose initial draft to a small fixed width (maybe even force it to be fixed) representation and then expanding back out into the final work

In ML the latent representation can be used as a feature.  For writing, the generated sample will be useful (the latent representation will perhaps be bullet points that are not suitable for the audience).

---

## Exploration vs explotiatiaon

https://arstechnica.com/science/2018/10/what-watching-forrest-gump-tells-us-about-how-we-store-memories/

---

## credit assignment

We don't know what experiences are positive or negative
- sometimes the most painful experience has the most benefit long term
- we only know the instantaneous pain/pleasure

THis relates to the instantaneous reward versus value function

ie even for a giben experience tuple, our value function changing means we will look at that sample of experience differently

page 119 how not to give a fuck 
Because here’s something that’s weird but true: we don’t actually know what a positive or negative experience is. Some of the most difficult and stressful moments of our lives also end up being the most formative and motivating. Some of the best and most gratifying experiences of our lives are also the most distracting and demotivating. Don’t trust your conception of positive/negative experiences. All that we know for certain is what hurts in the moment and what doesn’t. And that’s not worth much.

Manson, Mark. The Subtle Art of Not Giving a Fuck: A Counterintuitive Approach to Living a Good Life (p. 119). HarperCollins. Kindle Edition. 

Idea that this is the same in RL - when we sample experience we only know the instantaneous reward, we dont know what value this experience will have for learning (ie the td error :))


